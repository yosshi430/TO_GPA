{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ginza spacy\n",
        "!pip install ja-ginza\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFnTibZKbxsE",
        "outputId": "2cf0704b-0925-4604-81eb-dea936ff7017",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ginza in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: plac>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from ginza) (1.4.3)\n",
            "Requirement already satisfied: SudachiPy<0.7.0,>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from ginza) (0.6.8)\n",
            "Requirement already satisfied: SudachiDict-core>=20210802 in /usr/local/lib/python3.10/dist-packages (from ginza) (20240716)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ja-ginza in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from ja-ginza) (3.7.5)\n",
            "Requirement already satisfied: sudachipy<0.7.0,>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from ja-ginza) (0.6.8)\n",
            "Requirement already satisfied: sudachidict-core>=20210802 in /usr/local/lib/python3.10/dist-packages (from ja-ginza) (20240716)\n",
            "Requirement already satisfied: ginza<5.3.0,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from ja-ginza) (5.2.0)\n",
            "Requirement already satisfied: plac>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from ginza<5.3.0,>=5.2.0->ja-ginza) (1.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja-ginza) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ja-ginza) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja-ginza) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja-ginza) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja-ginza) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja-ginza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja-ginza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja-ginza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja-ginza) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ja-ginza) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ja-ginza) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja-ginza) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja-ginza) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.4.4->ja-ginza) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ja-ginza) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja-ginza) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja-ginza) (0.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoX37sXvDzze",
        "outputId": "5a32c09a-ce74-4b36-a987-41601522fb8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ja_core_news_smをダウンロード\n",
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "tQlzoNVFx-DO",
        "outputId": "4ad7922c-f474-4f59-a6e6-c312bf8df7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ja-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_sm-3.7.0/ja_core_news_sm-3.7.0-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: sudachipy!=0.6.1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-sm==3.7.0) (0.6.8)\n",
            "Requirement already satisfied: sudachidict-core>=20211220 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-sm==3.7.0) (20240716)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ja-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: ja-core-news-sm\n",
            "Successfully installed ja-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ja_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -Y\n",
        "!pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dcnTK9Y5v_yo",
        "outputId": "acb4656c-8def-42f1-a97e-27b593508e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 uninstall [options] <package> ...\n",
            "  pip3 uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -Y\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.0.50\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.0.50:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.0.50\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchgen"
                ]
              },
              "id": "b68ac9bd3cfd4f408a206343aab20550"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#チャットボット\n",
        "import os\n",
        "import openai\n",
        "import whisper\n",
        "from IPython.display import display, Audio\n",
        "import wave\n",
        "import pyaudio\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import time\n",
        "import pyopenjtalk\n",
        "from scipy.io import wavfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import serial\n",
        "import urllib.request\n",
        "import requests\n",
        "\n",
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "TOKEN = ''\n",
        "api_url = 'https://notify-api.line.me/api/notify'\n",
        "\n",
        "request_headers = {\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "    'Authorization': 'Bearer ' + TOKEN\n",
        "}\n",
        "\n",
        "def rms(data):\n",
        "    \"\"\"データのRMS（平均二乗平方根）を計算\"\"\"\n",
        "    samples = np.frombuffer(data, dtype=np.int16)\n",
        "    return np.sqrt(np.mean(np.square(samples)))\n",
        "\n",
        "def record():\n",
        "    # 音声の録音と保存\n",
        "    CHUNK = 4096  # 1度にどれくらい音を録るか\n",
        "    FORMAT = pyaudio.paInt16\n",
        "    CHANNELS = 1  # モノラル録音\n",
        "    RATE = 48000    #44100  # サンプリングレート\n",
        "    SILENCE_THRESHOLD = 40  # 音量しきい値（適切な値に調整してください）\n",
        "    FILENAME = \"Music/output.wav\"\n",
        "    SILENCE_DURATION = 3  # 録音停止するまでの無音の秒数\n",
        "\n",
        "    p = pyaudio.PyAudio()\n",
        "    frames = []\n",
        "    stream = p.open(\n",
        "        format=FORMAT,\n",
        "        channels=CHANNELS,\n",
        "        rate=RATE,\n",
        "        input=True,\n",
        "        frames_per_buffer=CHUNK\n",
        "    )\n",
        "\n",
        "    print('Recording...')\n",
        "    try:\n",
        "        silence_start_time = None\n",
        "        was_louder = False  # 音が一定のしきい値を超えたかどうか\n",
        "\n",
        "        while True:\n",
        "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
        "            frames.append(data)\n",
        "            current_rms = rms(data)\n",
        "            # print(f\"Current RMS: {current_rms}\")  # デバッグ用\n",
        "\n",
        "            if current_rms > SILENCE_THRESHOLD:#なぜか大きすぎると値が小さくなるからこうなってる（つまりこれは静かだったら）\n",
        "                if was_louder:\n",
        "                    if silence_start_time is None:\n",
        "                        silence_start_time = time.time()\n",
        "                        global timestamp\n",
        "                        timestamp = datetime.now().isoformat()\n",
        "                    elif time.time() - silence_start_time > SILENCE_DURATION:\n",
        "                        break\n",
        "            else:\n",
        "                was_louder = True\n",
        "                silence_start_time = None  # 音がある状態なので無音カウントをリセット\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Recording interrupted.\")\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    p.terminate()\n",
        "\n",
        "    # 録音データをファイルに保存\n",
        "    with wave.open(FILENAME, 'wb') as wf:\n",
        "        wf.setnchannels(CHANNELS)\n",
        "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
        "        wf.setframerate(RATE)\n",
        "        wf.writeframes(b''.join(frames))\n",
        "\n",
        "\n",
        "    audio_file= open(\"Music/output.wav\", \"rb\")\n",
        "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, language=\"ja\")\n",
        "    audio_file.close()\n",
        "    #print(transcript[\"text\"])\n",
        "    return transcript[\"text\"]\n",
        "\n",
        "def completion(new_message_text:str, settings_text:str = '', past_messages:list = []):\n",
        "    \"\"\"\n",
        "    This function generates a response message using OpenAI's GPT-3 model by taking in a new message text,\n",
        "    optional settings text and a list of past messages as inputs.\n",
        "\n",
        "    Args:\n",
        "    new_message_text (str): The new message text which the model will use to generate a response message.\n",
        "    settings_text (str, optional): The optional settings text that will be added as a system message to the past_messages list. Defaults to ''.\n",
        "    past_messages (list, optional): The optional list of past messages that the model will use to generate a response message. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the response message text and the updated list of past messages after appending the new and response messages.\n",
        "    \"\"\"\n",
        "    if len(past_messages) == 0 and len(settings_text) != 0:\n",
        "        system = {\"role\": \"system\", \"content\": settings_text}\n",
        "        past_messages.append(system)\n",
        "    new_message = {\"role\": \"user\", \"content\": new_message_text}\n",
        "    past_messages.append(new_message)\n",
        "\n",
        "    result = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=past_messages,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    response_message = {\"role\": \"assistant\", \"content\": result.choices[0].message.content}\n",
        "    past_messages.append(response_message)\n",
        "    response_message_text = result.choices[0].message.content\n",
        "    return response_message_text, past_messages\n",
        "# Most of the code are coppied from https://qiita.com/sakasegawa/items/db2cff79bd14faf2c8e0\n",
        "\n",
        "with open(\"Documents/chat_history.txt\", mode = \"r\", encoding=\"utf_8\") as f:\n",
        "    history = f.read()\n",
        "\n",
        "character_settings = \"\"\"あなたは少年です。大人との対話のシミュレーションを行ってもらいます。\n",
        "少年の特徴を以下に列挙します。\n",
        "\n",
        "小学四年生程度の知能である\n",
        "好奇心が旺盛である\n",
        "よき友人である\n",
        "生き生きとしている\n",
        "非常にコミュニケーション能力が高い\n",
        "会話を終えるときは「バイバイ」、「またね」などと言う\n",
        "\n",
        "上記例を参考に、性格や口調、言葉の作り方を模倣し、回答を構築してください。\n",
        "\"\"\"\n",
        "\n",
        "history_settings = f\"\"\"また、ずんだもんは過去に以下のようなやり取りを行っています\n",
        "{history}\n",
        "\"\"\"\n",
        "\n",
        "system_settings = character_settings + history_settings + \"ではシミュレーションを開始します。\"\n",
        "\n",
        "abbreviation_settings = \"\"\"会話の内容を簡潔にまとめてください。\n",
        "\"\"\"\n",
        "def contains_bye(new_message):\n",
        "    return \"バイバイ\" in new_message or \"また会おう\" in new_message or \"またね\" in new_message or \"さようなら\" in new_message\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    \"\"\"\n",
        "    指定されたディレクトリ内のファイルの数を返す関数。\n",
        "\n",
        "    :param directory_path: ファイルの数をカウントするディレクトリのパス\n",
        "    :return: ディレクトリ内のファイルの数\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ディレクトリ内のファイルのリストを取得\n",
        "        files = os.listdir(directory_path)\n",
        "\n",
        "        # ファイルのみをカウント（ディレクトリは除外）\n",
        "        file_count = sum(1 for f in files if os.path.isfile(os.path.join(directory_path, f)))\n",
        "\n",
        "        return file_count\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The directory {directory_path} does not exist.\")\n",
        "        return 0\n",
        "    except PermissionError:\n",
        "        print(f\"Permission denied for accessing {directory_path}.\")\n",
        "        return 0\n",
        "\n",
        "def json_create():\n",
        "    directory_path = \"conversation\"\n",
        "    dialogue_id = count_files_in_directory(directory_path)\n",
        "\n",
        "    empty_data = {#jsonファイルを作成\n",
        "        \"dialogue_id\": dialogue_id,\n",
        "        \"interlocutors\": [\n",
        "            \"USER\",\n",
        "            \"AI\"\n",
        "        ],\n",
        "        \"utterances\": [],  # 空の発話リスト\n",
        "        \"evaluations\": [\n",
        "            {\n",
        "                \"interlocutor_id\": \"USER\",\n",
        "                \"informativeness\": 5,\n",
        "                \"comprehension\": 5,\n",
        "                \"familiarity\": 5,\n",
        "                \"interest\": 5,\n",
        "                \"proactiveness\": 5,\n",
        "                \"satisfaction\": 5\n",
        "            },\n",
        "            {\n",
        "                \"interlocutor_id\": \"AI\",\n",
        "                \"informativeness\": 5,\n",
        "                \"comprehension\": 5,\n",
        "                \"familiarity\": 5,\n",
        "                \"interest\": 5,\n",
        "                \"proactiveness\": 5,\n",
        "                \"satisfaction\": 5\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    filename = str(datetime.now().isoformat())\n",
        "    global json_file\n",
        "    json_file = \"conversation/\" + filename + \".json\"\n",
        "\n",
        "    # JSONファイルとして保存\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(empty_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"まっさらなJSONファイルを作成しました。\")\n",
        "\n",
        "def json_add(interlocutor_id,utterance_text,time):\n",
        "    global json_file\n",
        "    # 既存のJSONファイルを読み込み\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 新しい発話のデータを作成\n",
        "    new_utterance = {\n",
        "        \"utterance_id\": len(data['utterances']),  # 新しいIDを設定\n",
        "        \"interlocutor_id\": interlocutor_id,\n",
        "        \"text\": utterance_text,\n",
        "        \"timestamp\": time  # 現在の日時をISOフォーマットで取得\n",
        "    }\n",
        "    # 'utterances'リストに新しい発話を追加\n",
        "    data['utterances'].append(new_utterance)\n",
        "\n",
        "    # 更新されたデータをJSONファイルに保存\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"新しい発話を追加しました。\")\n",
        "\n",
        "\n",
        "def line_danger():\n",
        "    contents = '\\n利用者さんが、気温の著しく低い、または高い部屋にいます。\\n危険です。'\n",
        "    payload = {'message': contents}\n",
        "    data = urllib.parse.urlencode(payload).encode('ascii')\n",
        "    print(data)\n",
        "    req = urllib.request.Request(api_url, headers=request_headers, data=data, method='POST')\n",
        "    conn = urllib.request.urlopen(req)\n",
        "\n",
        "def line_notice():\n",
        "    contents = '\\n利用者さんが認知症の疑いがあります。\\n病院で正確な診療を受けることをお勧めします'\n",
        "    payload = {'message': contents}\n",
        "    data = urllib.parse.urlencode(payload).encode('ascii')\n",
        "    print(data)\n",
        "    req = urllib.request.Request(api_url, headers=request_headers, data=data, method='POST')\n",
        "    conn = urllib.request.urlopen(req)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chatInit():\n",
        "    cnt = 0\n",
        "    json_create()\n",
        "    print(\"Press ctrl + C to end conversation\")\n",
        "    while True:\n",
        "            try:#会話の繰り返し\n",
        "                if cnt == 0:\n",
        "                    response = record()\n",
        "                    user_text = \"You: \" + response\n",
        "                    print(user_text)\n",
        "                    new_message, messages = completion(user_text, system_settings, [])\n",
        "                    AI_text = \"AI: \" + new_message\n",
        "                    print(AI_text)\n",
        "                    cnt += 1\n",
        "                else:\n",
        "                    response = record()\n",
        "                    user_text = \"You: \" + response\n",
        "                    print(user_text)\n",
        "                    new_message, messages = completion(user_text, system_settings, messages)\n",
        "                    AI_text = \"AI: \" + new_message\n",
        "                    print(AI_text)\n",
        "\n",
        "                x, sr = pyopenjtalk.tts(new_message)\n",
        "                wavfile.write(\"Music/response.wav\", sr, x.astype(np.int16))\n",
        "                FILENAME = \"Music/response.wav\"\n",
        "                # 音声ファイルをロード\n",
        "                sound = AudioSegment.from_file(FILENAME)\n",
        "\n",
        "                print(\"stopping Conversation...\")\n",
        "                timestamp_response = datetime.now().isoformat()#返答のタイムスタンプ\n",
        "                # 再生\n",
        "                play(sound)\n",
        "\n",
        "                with open(\"Documents/conversation.txt\", mode=\"a\", encoding=\"utf_8\") as f:#会話履歴をそのまま蓄積+AIの話終わりの時間\n",
        "                    f.write(user_text + \"\\n\" + AI_text + \"\\n\")\n",
        "\n",
        "                json_add(\"USER\",response,timestamp)#jsonファイルに会話を保存\n",
        "                json_add(\"AI\",new_message,timestamp_response)\n",
        "\n",
        "                if contains_bye(new_message):#会話にバイバイがあったら\n",
        "                    print(\"Ending Conversation...\")\n",
        "                    break\n",
        "\n",
        "                print(\"starting Conversation...\")\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"Ending Conversation...\")\n",
        "                break\n",
        "    chat_history, _ = completion(str(messages), abbreviation_settings, [])#今回の会話を要約\n",
        "    with open(\"Documents/chat_history.txt\", mode=\"a\", encoding=\"utf_8\") as f:\n",
        "        f.write(chat_history + \"\\n\")\n",
        "\n",
        "    with open(\"Documents/chat_history.txt\", mode=\"r\", encoding=\"utf_8\") as f:\n",
        "        history = f.read()\n",
        "    chat_history, _ = completion(history, abbreviation_settings, [])#今までの会話を要約\n",
        "    with open(\"Documents/chat_history.txt\", mode=\"w\", encoding=\"utf_8\") as f:\n",
        "        f.write(chat_history)\n",
        "\n",
        "    #学習モデルに異常かどうか判定させて、lineを送信する。\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ser = serial.Serial('/dev/ttyUSB0', 9600, timeout=1)\n",
        "    ser.flush()\n",
        "    while True:\n",
        "        if ser.in_waiting > 0:\n",
        "            line = ser.readline().decode('utf-8').rstrip()\n",
        "            print(line)\n",
        "            if line == \"Start conversation\":\n",
        "                chatInit()\n",
        "            elif line == \"Danger!\":\n",
        "                line_danger()"
      ],
      "metadata": {
        "id": "-8XY-N3sv3gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#異常判定\n",
        "import ginza\n",
        "import spacy\n",
        "import json\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "\n",
        "# GINZAを読み込む\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# 学習済みモデルを読み込む\n",
        "model_file_path = \"svm_model.pkl\"\n",
        "loaded_model = joblib.load(model_file_path)\n",
        "\n",
        "# 特徴量のスケーラーを読み込む\n",
        "scaler_file_path = \"scaler.pkl\"\n",
        "scaler = joblib.load(scaler_file_path)\n",
        "\n",
        "# JSONファイルのパス\n",
        "json_file_path = \"/content/00001.json\"  # ここに新しいJSONファイルのパスを指定\n",
        "\n",
        "# JSONデータの読み込み\n",
        "with open(json_file_path, \"r\", encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "utterances = [u['text'] for u in data['utterances']]\n",
        "timestamps_str = [u['timestamp'] for u in data['utterances']]\n",
        "\n",
        "print(utterances)\n",
        "\n",
        "# ISOフォーマット文字列からdatetimeオブジェクトに変換\n",
        "timestamps = []\n",
        "for ts in timestamps_str:\n",
        "    try:\n",
        "        ts_dt = datetime.fromisoformat(ts)\n",
        "        timestamps.append(ts_dt)\n",
        "    except ValueError:\n",
        "        timestamps.append(None)\n",
        "\n",
        "# タイムスタンプに無効な値が含まれている場合は処理を中止\n",
        "if None in timestamps:\n",
        "    print(\"無効なタイムスタンプが含まれています。処理を中止します。\")\n",
        "    exit()\n",
        "\n",
        "# 各発話を解析し、結果を計算する\n",
        "doc = nlp(\" \".join(utterances))\n",
        "\n",
        "# 各品詞のカウントと割合計算\n",
        "pos_counts = Counter([token.pos_ for token in doc])\n",
        "total_tokens = sum(pos_counts.values())\n",
        "\n",
        "# 一般名詞割合\n",
        "noun_counts = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
        "common_noun_ratio = noun_counts / total_tokens\n",
        "\n",
        "# 固有名詞割合\n",
        "proper_noun_counts = sum(1 for token in doc if token.pos_ == \"PROPN\")\n",
        "proper_noun_ratio = proper_noun_counts / total_tokens\n",
        "\n",
        "# 最大係り受け距離\n",
        "dependency_distances = [abs(token.head.i - token.i) for token in doc]\n",
        "max_dependency_distance = max(dependency_distances)\n",
        "\n",
        "# TTR（Type Token Ratio）\n",
        "unique_tokens = len(set([token.text for token in doc]))\n",
        "ttr = unique_tokens / total_tokens\n",
        "\n",
        "# 異なり名詞割合\n",
        "unique_nouns = len(set([token.text for token in doc if token.pos_ == \"NOUN\"]))\n",
        "unique_noun_ratio = unique_nouns / total_tokens\n",
        "\n",
        "# 代名詞割合\n",
        "pronoun_counts = sum(1 for token in doc if token.pos_ == \"PRON\")\n",
        "pronoun_ratio = pronoun_counts / total_tokens\n",
        "\n",
        "# フィラー割合\n",
        "filler_counts = sum(1 for token in doc if token.text in [\"えー\", \"あー\", \"うーん\", \"えっと\"])\n",
        "filler_ratio = filler_counts / total_tokens\n",
        "\n",
        "# 反応時間計算\n",
        "reaction_times = [(timestamps[i+1] - timestamps[i]).total_seconds() for i in range(len(timestamps)-1)]\n",
        "#average_reaction_time = np.mean(reaction_times)\n",
        "average_reaction_time = 50\n",
        "variance_reaction_time = np.var(reaction_times)\n",
        "\n",
        "# 特徴量をリストにまとめる\n",
        "features = np.array([[common_noun_ratio, proper_noun_ratio, max_dependency_distance,\n",
        "                      ttr, unique_noun_ratio, pronoun_ratio,\n",
        "                      filler_ratio, average_reaction_time, variance_reaction_time]])\n",
        "\n",
        "#features = np.array([[ average_reaction_time]])\n",
        "\n",
        "# 特徴量のスケーリング\n",
        "features_scaled = scaler.transform(features)\n",
        "\n",
        "# モデルを使って予測\n",
        "prediction = loaded_model.predict(features_scaled)\n",
        "\n",
        "# 異常かどうかを表示\n",
        "if prediction[0] == -1:\n",
        "    print(\"このデータは異常です。\")\n",
        "else:\n",
        "    print(\"このデータは正常です。\")\n"
      ],
      "metadata": {
        "id": "ER4GPMHtv9EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7e18b7-62e5-41ba-ebaf-43402291b6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['よろしくお願いいたします。', 'よろしくお願いします！', '今日は涼しいですね', '雨が降って、何か涼しくなりましたね。', 'そうですね、明日も涼しいと聞きました', 'そうなんですか！でも、ちょっと湿度が高い気がします。', '確かに、雨の名残でしょうか', '今日の天気はどうでしたか？', '私のところは曇りでした', 'そうなんですね！関東地方とかですか？', 'そうです！都内にいます', '関東は曇りなんですね。こちらは関西です。', '関西良いですね！お天気はどうでしょうか？', '午前中は、雷が鳴って、土砂降りでした。', 'えっ、それは。明日関東も雨かもしれませんね', '行きますね、その雨雲が。', 'プレゼントみたいですね', '嫌なプレゼントだー！今はカンカン照りです。', 'えっ、そしたら月曜日晴れちゃいますね', 'もう暑いのは勘弁です。夏バテしてませんか？', 'してます、してます。もうぐったりです', '食欲はどうですか？', '食欲もそんなにわかず。', 'そういう時は、何か食べますか？私は麺類が多めになります。', '分かります、後はグラノーラ食べますね', '健康的ですね！グラノーラの存在すっかり忘れてました。', 'グラノーラ地味に大切な存在です。', '栄養素たっぷりですもんね！私も、買っておこうかな。', 'おすすめします！パットしたときに食べられるので、ぜひ', 'ナイスな情報ありがとうございます！ではまた。']\n",
            "このデータは正常です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "global json_file\n",
        "json_file = \"/content/00001 (1).json\"\n",
        "\n",
        "def load_categories(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        categories = [line.strip() for line in file]\n",
        "    return categories\n",
        "\n",
        "def create_prompt(conversation_text, categories):\n",
        "    prompt = f\":\\n\\n会話:\\n{conversation_text}\\n\\nカテゴリ候補:\\n\"\n",
        "    for i, category in enumerate(categories, 1):\n",
        "        prompt += f\"- {category}\\n\"\n",
        "    prompt += \"\\n最も関連性の高いカテゴリを上位3つをjsonで返してください。\"\n",
        "    return prompt\n",
        "\n",
        "def get_similarity(json_file_name):\n",
        "    GPT_ENDPOINT = 'https://api.openai.com/v1/chat/completions'\n",
        "    API_TOKEN = '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'\n",
        "    MODEL_NAME = 'gpt-4o'\n",
        "    MAX_TOKENS = 500\n",
        "    MODEL_TEMP = 0.7\n",
        "\n",
        "    txt_file_name = \"/content/category_data.txt\"\n",
        "\n",
        "    with open(json_file_name, 'r', encoding='utf-8') as file:\n",
        "        conversation_json = json.load(file)\n",
        "\n",
        "    system_prompt = \"会話がどのカテゴリに関連しているかを評価してください。会話の雰囲気やスタイルで判断せず、会話の内容のみで判断してください。結果は必ずjsonで返してください。jsonには確率を数値で、評価の理由をコメントで含んでください。\"\n",
        "\n",
        "    assistant_prompt = 'それぞれのjsonは次の形式で、最も確率が高いものから返してください。\"categories\": [ {\"category\": \"\",\"probability\":\" \",reason\": \"\"}]'\n",
        "\n",
        "    categories = load_categories(txt_file_name)\n",
        "\n",
        "    conversation_text = \" \".join([utterance[\"text\"] for utterance in conversation_json[\"utterances\"]])\n",
        "\n",
        "    prompt = create_prompt(conversation_text, categories)\n",
        "\n",
        "    messages = [\n",
        "        { 'role': 'system', 'content': system_prompt },\n",
        "        { 'role': 'assistant', 'content': assistant_prompt },\n",
        "        { 'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {API_TOKEN}',\n",
        "        'Content-Type': 'application/json',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        'model': MODEL_NAME,\n",
        "        'max_tokens': MAX_TOKENS,\n",
        "        'temperature': MODEL_TEMP,\n",
        "        'messages': messages,\n",
        "        'response_format': { 'type': 'json_object' }\n",
        "    }\n",
        "\n",
        "    response = requests.post(GPT_ENDPOINT, headers=headers, json=data)\n",
        "    response_json = response.json()\n",
        "    json_data = response_json['choices'][0]['message']['content'].strip()\n",
        "    return json_data\n",
        "\n",
        "def category_json_record():\n",
        "    global json_file\n",
        "    # json_file_nameはファイルパスとして文字列で渡す\n",
        "    json_file_name = json_file\n",
        "\n",
        "    data = get_similarity(json_file_name)\n",
        "    print(data)\n",
        "    with open('output.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "category_json_record()\n"
      ],
      "metadata": {
        "id": "m_TT_RUtv9KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#検証用\n",
        "import shutil\n",
        "shutil.rmtree('/content/非認知症')\n"
      ],
      "metadata": {
        "id": "xGZkXGiDMsrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "Bb8FlCiTNsl1",
        "outputId": "aadfe6a0-96ee-435a-fc2b-135e62b85f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6dcf1435-0cfe-48a1-84fb-9200fdf959d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6dcf1435-0cfe-48a1-84fb-9200fdf959d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving transcript_S.zip to transcript_S.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip transcript_S.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yUlOaUfTNvSL",
        "outputId": "9a84e868-2b37-4c51-b2e7-ca32000629b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  transcript_S.zip\n",
            "   creating: transcript_S/ФFТmП╟/\n",
            "   creating: transcript_S/ФFТmП╟/S17/\n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S17/S17_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S21/\n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S21/S21_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S24/\n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S24/S24_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S26/\n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S26/S26_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S32/\n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S32/S32_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S34/\n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_01_EP1a.txt  \n",
            " extracting: transcript_S/ФFТmП╟/S34/S34_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S34/S34_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S37/\n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S37/S37_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S39/\n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S39/S39_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S41/\n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S41/S41_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S42/\n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S42/S42_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S44/\n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S44/S44_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S50/\n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S50/S50_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S51/\n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S51/S51_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S52/\n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S52/S52_03_01_AM.txt  \n",
            "   creating: transcript_S/ФFТmП╟/S62/\n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФFТmП╟/S62/S62_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/\n",
            "   creating: transcript_S/ФёФFТmП╟/S01/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S01/S01_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S02/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_01_EP1a.txt  \n",
            " extracting: transcript_S/ФёФFТmП╟/S02/S02_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S02/S02_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S03/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S03/S03_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S05/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S05/S05_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S06/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S06/S06_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S07/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S07/S07_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S08/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S08/S08_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S10/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S10/S10_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S11/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S11/S11_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S12/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S12/S12_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S13/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S13/S13_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S14/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S14/S14_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S16/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S16/S16_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S20/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_01_EP1a.txt  \n",
            " extracting: transcript_S/ФёФFТmП╟/S20/S20_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S20/S20_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S22/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S22/S22_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S23/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S23/S23_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S25/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S25/S25_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S27/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S27/S27_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S28/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S28/S28_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S29/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S29/S29_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S30/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S30/S30_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S31/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S31/S31_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S33/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_01_EP1a.txt  \n",
            " extracting: transcript_S/ФёФFТmП╟/S33/S33_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S33/S33_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S35/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S35/S35_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S36/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S36/S36_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S38/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S38/S38_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S40/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S40/S40_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S43/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S43/S43_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S45/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S45/S45_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S46/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S46/S46_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S47/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S47/S47_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S48/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S48/S48_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S49/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S49/S49_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S53/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S53/S53_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S54/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S54/S54_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S55/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S55/S55_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S56/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S56/S56_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S57/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S57/S57_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S58/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S58/S58_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S59/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S59/S59_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S60/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S60/S60_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S61/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S61/S61_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S63/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S63/S63_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S64/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S64/S64_03_01_AM.txt  \n",
            "   creating: transcript_S/ФёФFТmП╟/S65/\n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_01_EP1a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_02_EP1b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_03_EP2a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_04_EP3a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_05_EP4a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_06_EP5a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_07_EP6a.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_08_EP6b.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_09_EP7.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_01_10_EP8.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_02_01_ST.txt  \n",
            "  inflating: transcript_S/ФёФFТmП╟/S65/S65_03_01_AM.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルのパス\n",
        "txt_file_path = \"/content/transcript_S/ФFТmП╟/S17/S17_01_01_EP1a.txt\"  # .txtファイルのパス\n",
        "\n",
        "# テキストファイルの読み込み\n",
        "with open(txt_file_path, \"r\", encoding='UTF-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkGfS2dQJ9pD",
        "outputId": "bb4484c1-f1a6-457a-e1b7-69237822a9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿<e>\n",
            "今から１年以上前なんですが　えー　父親の弟　いわゆる叔父ですね　えー　この方が亡くなりまして　えー　その１年前にも叔母　が亡くなっ　あの　夫婦で　えー　連続して　亡くなったんですが　え　よく　わたくし　かわいがってもらったので　えー　もうだいぶ　年を取ってたので　えー　寿命かなとは思ったんですけど　えー　とても悲しく思いました\n",
            "えー　娘さんが３人いらっしゃいまして　えー　皆さん本当に悲しそうでした\n",
            "えー　よく　娘さんたちが　その叔父さんを　介護して　仕えたと　感心しております\n",
            "以上　です\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 判定用真（.txt）\n",
        "import ginza\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "\n",
        "# GINZAを読み込む\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# 学習済みモデルを読み込む\n",
        "model_file_path = \"/content/txt_svm_model.pkl\"\n",
        "loaded_model = joblib.load(model_file_path)\n",
        "\n",
        "# 特徴量のスケーラーを読み込む\n",
        "scaler_file_path = \"/content/txt_scaler.pkl\"\n",
        "scaler = joblib.load(scaler_file_path)\n",
        "\n",
        "# ファイルのパス\n",
        "txt_file_path = \"/content/drive/MyDrive/対象群コーパス/対照群コーパス/J01/J01_01_01_EP1a.txt\"  # .txtファイルのパス\n",
        "\n",
        "# テキストファイルの読み込み\n",
        "with open(txt_file_path, \"r\", encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# テキストを文ごとに分割（ここでは改行で分割）\n",
        "utterances = text.splitlines()\n",
        "\n",
        "# 各発話を解析し、結果を計算する\n",
        "doc = nlp(\" \".join(utterances))\n",
        "\n",
        "# 各品詞のカウントと割合計算\n",
        "pos_counts = Counter([token.pos_ for token in doc])\n",
        "total_tokens = sum(pos_counts.values())\n",
        "\n",
        "# 一般名詞割合\n",
        "noun_counts = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
        "common_noun_ratio = noun_counts / total_tokens\n",
        "\n",
        "# 固有名詞割合\n",
        "proper_noun_counts = sum(1 for token in doc if token.pos_ == \"PROPN\")\n",
        "proper_noun_ratio = proper_noun_counts / total_tokens\n",
        "\n",
        "# 最大係り受け距離\n",
        "dependency_distances = [abs(token.head.i - token.i) for token in doc]\n",
        "max_dependency_distance = max(dependency_distances)\n",
        "\n",
        "# TTR（Type Token Ratio）\n",
        "unique_tokens = len(set([token.text for token in doc]))\n",
        "ttr = unique_tokens / total_tokens\n",
        "\n",
        "# 異なり名詞割合\n",
        "unique_nouns = len(set([token.text for token in doc if token.pos_ == \"NOUN\"]))\n",
        "unique_noun_ratio = unique_nouns / total_tokens\n",
        "\n",
        "# 代名詞割合\n",
        "pronoun_counts = sum(1 for token in doc if token.pos_ == \"PRON\")\n",
        "pronoun_ratio = pronoun_counts / total_tokens\n",
        "\n",
        "# フィラー割合\n",
        "filler_counts = sum(1 for token in doc if token.text in [\"えー\", \"あー\", \"うーん\", \"えっと\"])\n",
        "filler_ratio = filler_counts / total_tokens\n",
        "\n",
        "\n",
        "\n",
        "# 特徴量をリストにまとめる\n",
        "features = np.array([[common_noun_ratio, proper_noun_ratio, max_dependency_distance,\n",
        "                      ttr, unique_noun_ratio, pronoun_ratio,filler_ratio]])\n",
        "\n",
        "# 特徴量のスケーリング\n",
        "features_scaled = scaler.transform(features)\n",
        "\n",
        "# モデルを使って予測\n",
        "prediction = loaded_model.predict(features_scaled)\n",
        "\n",
        "# 異常かどうかを表示\n",
        "if prediction[0] == -1:\n",
        "    print(\"このデータは異常です。\")\n",
        "else:\n",
        "    print(\"このデータは正常です。\")\n"
      ],
      "metadata": {
        "id": "n6tha_64D8Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec39a563-a19e-424f-ef26-c5fdcf1584b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "このデータは正常です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#フォルダごとに判断\n",
        "import os\n",
        "import ginza\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "seijou = 0\n",
        "ijou = 0\n",
        "\n",
        "# GINZAを読み込む\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# 学習済みモデルを読み込む\n",
        "model_file_path = \"/content/txt_exact_svm_model.pkl\"\n",
        "loaded_model = joblib.load(model_file_path)\n",
        "\n",
        "# 特徴量のスケーラーを読み込む\n",
        "scaler_file_path = \"/content/txt_exact_scaler.pkl\"\n",
        "scaler = joblib.load(scaler_file_path)\n",
        "\n",
        "# フォルダのパス\n",
        "folder_path = \"/content/drive/MyDrive/対象群コーパス/対照群コーパス/S21\"  # 処理対象のフォルダ\n",
        "\n",
        "# フォルダ内の全てのファイルを取得\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # テキストファイルの読み込み\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            #print(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "     # ファイルの内容を解析\n",
        "    doc = nlp(text)\n",
        "    #print(f\"解析結果: {doc[:10]}...\")  # 最初の10トークンだけ表示\n",
        "\n",
        "    # 各品詞のカウントと割合計算\n",
        "    pos_counts = Counter([token.pos_ for token in doc])\n",
        "    total_tokens = sum(pos_counts.values())\n",
        "    if total_tokens == 0:\n",
        "        print(f\"ファイル {file_path} は有効なトークンがありません。スキップします。\")\n",
        "        continue\n",
        "\n",
        "    pos_ratios = {pos: count / total_tokens for pos, count in pos_counts.items()}\n",
        "\n",
        "    # 一般名詞割合\n",
        "    noun_counts = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
        "    common_noun_ratio = noun_counts / total_tokens\n",
        "\n",
        "    # 固有名詞割合\n",
        "    proper_noun_counts = sum(1 for token in doc if token.pos_ == \"PROPN\")\n",
        "    proper_noun_ratio = proper_noun_counts / total_tokens\n",
        "\n",
        "    # 最大係り受け距離と平均係り受け距離\n",
        "    dependency_distances = [abs(token.head.i - token.i) for token in doc]\n",
        "    max_dependency_distance = max(dependency_distances)\n",
        "    average_dependency_distance = np.mean(dependency_distances)\n",
        "\n",
        "    # TTR（Type Token Ratio）\n",
        "    unique_tokens = len(set([token.text for token in doc]))\n",
        "    ttr = unique_tokens / total_tokens\n",
        "\n",
        "    # 異なり名詞割合\n",
        "    unique_nouns = len(set([token.text for token in doc if token.pos_ == \"NOUN\"]))\n",
        "    unique_noun_ratio = unique_nouns / total_tokens\n",
        "\n",
        "    # 代名詞割合\n",
        "    pronoun_counts = sum(1 for token in doc if token.pos_ == \"PRON\")\n",
        "    pronoun_ratio = pronoun_counts / total_tokens\n",
        "\n",
        "    # フィラー割合\n",
        "    filler_counts = sum(1 for token in doc if token.text in [\"えー\", \"あー\", \"うーん\", \"えっと\"])\n",
        "    filler_ratio = filler_counts / total_tokens\n",
        "\n",
        "\n",
        "\n",
        "    # 特徴量をリストにまとめる\n",
        "    features = np.array([[common_noun_ratio, proper_noun_ratio, max_dependency_distance,\n",
        "                          ttr, unique_noun_ratio, pronoun_ratio,filler_ratio]])\n",
        "\n",
        "    # 特徴量のスケーリング\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    # モデルを使って予測\n",
        "    prediction = loaded_model.predict(features_scaled)\n",
        "\n",
        "    # 異常かどうかを表示\n",
        "    if prediction[0] == -1:\n",
        "        print(f\"File {file_name}: このデータは異常です。\")\n",
        "        ijou +=1\n",
        "    else:\n",
        "        print(f\"File {file_name}: このデータは正常です。\")\n",
        "        seijou += 1\n",
        "\n",
        "print('正常：'+ str(seijou))\n",
        "print('異常：'+ str(ijou))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "o9hQ2BwTOGUc",
        "outputId": "18b6ac37-a624-4e85-87ba-e992058dd38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_C' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-37ef0de4e92b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#フォルダごとに判断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mginza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ginza/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msudachipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorpheme\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorpheme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mja\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetailedToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m__name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#フォルダごとに判断(全部一気に)\n",
        "import os\n",
        "import ginza\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "seijou = 0\n",
        "ijou = 0\n",
        "count = 0\n",
        "\n",
        "# GINZAを読み込む\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# 学習済みモデルを読み込む\n",
        "model_file_path = \"/content/txt_01_svm_model.pkl\"\n",
        "loaded_model = joblib.load(model_file_path)\n",
        "\n",
        "# 特徴量のスケーラーを読み込む\n",
        "scaler_file_path = \"/content/txt_01_scaler.pkl\"\n",
        "scaler = joblib.load(scaler_file_path)\n",
        "\n",
        "# 全部のやつ\n",
        "big_folder = \"/content/drive/MyDrive/対象群コーパス/仕分け済みコーパス/認知症\"\n",
        "\n",
        "# フォルダ内の全てのフォルダを取得\n",
        "folder_list = os.listdir(big_folder)\n",
        "\n",
        "# 各フォルダに対して処理を実行\n",
        "for folder_name in folder_list:\n",
        "    bigfolder_path = os.path.join(big_folder, folder_name)\n",
        "\n",
        "    # フォルダのパス\n",
        "    #folder_path = \"/content/drive/MyDrive/対象群コーパス/対照群コーパス/S02\"  # 処理対象のフォルダ\n",
        "\n",
        "    # フォルダ内の全てのファイルを取得\n",
        "    file_list = os.listdir(bigfolder_path)\n",
        "    seijou = 0\n",
        "    ijou = 0\n",
        "\n",
        "    # 各ファイルに対して処理を実行\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(bigfolder_path, file_name)\n",
        "\n",
        "        # テキストファイルの読み込み\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                #print(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # ファイルの内容を解析\n",
        "        doc = nlp(text)\n",
        "        #print(f\"解析結果: {doc[:10]}...\")  # 最初の10トークンだけ表示\n",
        "\n",
        "        # 各品詞のカウントと割合計算\n",
        "        pos_counts = Counter([token.pos_ for token in doc])\n",
        "        total_tokens = sum(pos_counts.values())\n",
        "        if total_tokens == 0:\n",
        "            print(f\"ファイル {file_path} は有効なトークンがありません。スキップします。\")\n",
        "            continue\n",
        "\n",
        "        pos_ratios = {pos: count / total_tokens for pos, count in pos_counts.items()}\n",
        "\n",
        "        # 一般名詞割合\n",
        "        noun_counts = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
        "        common_noun_ratio = noun_counts / total_tokens\n",
        "\n",
        "        # 固有名詞割合\n",
        "        proper_noun_counts = sum(1 for token in doc if token.pos_ == \"PROPN\")\n",
        "        proper_noun_ratio = proper_noun_counts / total_tokens\n",
        "\n",
        "        # 最大係り受け距離と平均係り受け距離\n",
        "        dependency_distances = [abs(token.head.i - token.i) for token in doc]\n",
        "        max_dependency_distance = max(dependency_distances)\n",
        "        average_dependency_distance = np.mean(dependency_distances)\n",
        "\n",
        "        # TTR（Type Token Ratio）\n",
        "        unique_tokens = len(set([token.text for token in doc]))\n",
        "        ttr = unique_tokens / total_tokens\n",
        "\n",
        "        # 異なり名詞割合\n",
        "        unique_nouns = len(set([token.text for token in doc if token.pos_ == \"NOUN\"]))\n",
        "        unique_noun_ratio = unique_nouns / total_tokens\n",
        "\n",
        "        # 代名詞割合\n",
        "        pronoun_counts = sum(1 for token in doc if token.pos_ == \"PRON\")\n",
        "        pronoun_ratio = pronoun_counts / total_tokens\n",
        "\n",
        "        # フィラー割合\n",
        "        filler_counts = sum(1 for token in doc if token.text in [\"えー\", \"あー\", \"うーん\", \"えっと\"])\n",
        "        filler_ratio = filler_counts / total_tokens\n",
        "\n",
        "\n",
        "\n",
        "        # 特徴量をリストにまとめる\n",
        "        features = np.array([[common_noun_ratio, proper_noun_ratio, max_dependency_distance,\n",
        "                              ttr, unique_noun_ratio, pronoun_ratio,filler_ratio]])\n",
        "\n",
        "        # 特徴量のスケーリング\n",
        "        features_scaled = scaler.transform(features)\n",
        "\n",
        "        # モデルを使って予測\n",
        "        prediction = loaded_model.predict(features_scaled)\n",
        "\n",
        "        # 異常かどうかを表示\n",
        "        if prediction[0] == -1:\n",
        "            #print(f\"File {file_name}: このデータは異常です。\")\n",
        "            ijou += 1\n",
        "        elif prediction[0] == 1:\n",
        "            #print(f\"File {file_name}: このデータは正常です。\")\n",
        "            seijou += 1\n",
        "\n",
        "    print(folder_name)\n",
        "\n",
        "    if ijou >= 2:\n",
        "      print(\"異常の可能性があります\")\n",
        "      count += 1\n",
        "    else:\n",
        "      print(\"正常\")\n",
        "\n",
        "print(\"異常の数：\"+str(count))\n",
        "#print('正常：'+ str(seijou))\n",
        "#print('異常：'+ str(ijou))"
      ],
      "metadata": {
        "id": "98e7qPEnT1Uq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e9c87350-cd30-4a44-e2da-dbb416bb66e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_C' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-801ce66fc6cc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#フォルダごとに判断(全部一気に)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mginza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ginza/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msudachipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorpheme\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorpheme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mja\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetailedToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m__name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m__name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHDGqA8sJHye"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}